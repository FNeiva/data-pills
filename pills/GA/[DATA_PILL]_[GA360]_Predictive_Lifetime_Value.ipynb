{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[DATA PILL] [GA360] - Predictive Lifetime Value.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG6_2HbY7X1H"
      },
      "source": [
        "**PLEASE MAKE A COPY BEFORE CHANGING**\n",
        "\n",
        "**Copyright** 2021 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "<b>Important</b>\n",
        "This content are intended for educational and informational purposes only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reE-i-9V7Efh"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "**objective**:  The goal of this colab is to calculate the Lifetime Value of your customer base. The method used for calculation is the BG/NDB model as described in this [paper](http://mktg.uni-svishtov.bg/ivm/resources/Counting_Your_Customers.pdf). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXoq1O3vSlj-"
      },
      "source": [
        "# Code Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwTtzedhvDvH"
      },
      "source": [
        "!pip install -q lifetimes\n",
        "!pip install -q --upgrade git+https://github.com/HIPS/autograd.git@master \n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxUJRrvUjUr4"
      },
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from lifetimes import BetaGeoFitter\n",
        "from lifetimes.plotting import plot_frequency_recency_matrix\n",
        "from lifetimes.plotting import plot_probability_alive_matrix\n",
        "from lifetimes.plotting import plot_period_transactions\n",
        "from lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases\n",
        "from lifetimes.plotting import plot_history_alive\n",
        "from lifetimes import GammaGammaFitter\n",
        "\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Allow plots to be displayed inline.\n",
        "%matplotlib inline\n",
        "\n",
        "# Authenticate the user\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3_-vXnmTIJ5"
      },
      "source": [
        "### Input your settings to run the model\n",
        "\n",
        "*   **project_id**: The id of Google Cloud project where the query will run.\n",
        "*   **table_name**: Name of the Google Analytics table\n",
        "transaction.\n",
        "*   **time_unit**: The granularity to group transactions (weeks is usually the best)\n",
        "*   **units_to_predict**: Number of periods to predict (in case of using weeks as time_unit, 52 would predict a year ahead)\n",
        "*   **number_of_segments**: Number segments to group the users in.\n",
        "*   **id_type**: Two options of IDs. client_id is based on GA cookie (not cross device and can change overtime. user_id can be provide better acurracy).\n",
        "*   **use_id_dimension_index**: The custom dimension index that is holding the user id value. If the id_type is user_id, this field is mandatory.\n",
        "*   **data_import_key_index**: The custom dimension index that is holding the user id. If you are not sure, don't worry, it can be changed later.\n",
        "*   **data_import_value_index**: The custom dimension index that is holding the LTV segment. If you are not sure, don't worry, it can be changed later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mlc3e_8S3qK"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YrNXngZtNNe"
      },
      "source": [
        "project_id = 'my-project'#@param\n",
        "table_name = 'bigquery-public-data.google_analytics_sample.ga_sessions_*'#@param\n",
        "time_unit = 'weeks'#@param ['days', 'weeks', 'months']\n",
        "units_to_predict = 52#@param\n",
        "start_date = '2018-01-01'#@param{type:\"date\"}\n",
        "end_date = '2019-01-01'#@param{type:\"date\"}\n",
        "number_of_segments = 5#@param\n",
        "id_type = 'client_id'#@param ['client_id', 'user_id']\n",
        "user_id_dimension_index = 0#@param\n",
        "data_import_key_index = 11#@param\n",
        "data_import_value_index = 12#@param\n",
        "\n",
        "dc = {}\n",
        "dc['start_date'] = start_date.replace('-', '')\n",
        "dc['end_date'] = end_date.replace('-', '')\n",
        "dc['table_name'] = table_name\n",
        "dc['id_type'] = id_type\n",
        "dc['user_id_dimension_index'] = user_id_dimension_index\n",
        "\n",
        "if time_unit == 'days':\n",
        "    dc['time_unit'] = 1\n",
        "elif time_unit == 'weeks' or time_unit == '':\n",
        "    dc['time_unit'] = 7\n",
        "else:\n",
        "    dc['time_unit'] = 12\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSrNvcEbWTOj"
      },
      "source": [
        "### Get data from BigQuery\n",
        "\n",
        "Query the Google Analytics 360 in BigQuery to create the RFM matrix containing the following columns.\n",
        "\n",
        "\n",
        "*   **user_id**: id of the user\n",
        "*   **total_transactions**: Ammount of transaction during the period.\n",
        "*   **average_order_value**: Sum of total_transaction_value / total_transactions\n",
        "*   **frequency**: Represents the number of repeat purchases the customer has made.\n",
        "*   **recency**: Represents the age of the customer when they made their most recent purchase. This is equal to the duration between a customer’s first purchase and their latest purchase. (Thus if they have made only 1 purchase, the recency is 0.)\n",
        "*   **T**: Represents the age of the customer in whatever time units chosen (weekly, in the above dataset). This is equal to the duration between a customer’s first purchase and the end of the period under study.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-HSwebe7zF9"
      },
      "source": [
        "q1 = \"\"\"\n",
        "\n",
        "WITH transactions as (\n",
        "SELECT\n",
        "  clientid AS user_id,\n",
        "  PARSE_DATE('%Y%m%d', date) AS transaction_date,\n",
        "  SUM(totals.totalTransactionRevenue) / 1000000 AS transaction_value\n",
        "FROM \n",
        "  `{table_name}`\n",
        "  WHERE\n",
        "    _TABLE_SUFFIX BETWEEN '{start_date}'\n",
        "    AND '{end_date}' AND totals.transactions > 0\n",
        "    AND totals.totalTransactionRevenue > 0\n",
        "GROUP BY\n",
        "  1, 2)\n",
        "  \n",
        "SELECT\n",
        "  user_id,\n",
        "  COUNT(1) AS total_transactions,\n",
        "  ROUND(SUM(transaction_value)/COUNT(1),1) AS average_order_value,\n",
        "  (COUNT(1)-1) AS frequency,  \n",
        "  ROUND (DATE_DIFF(MAX(transaction_date),\n",
        "            MIN(transaction_date),\n",
        "            DAY) / {time_unit} ,1) # time multiplyer \n",
        "    AS recency,\n",
        "\n",
        "  ROUND((DATE_DIFF((SELECT MAX(transaction_date) FROM transactions),\n",
        "             MIN(transaction_date),\n",
        "             DAY)+1) / {time_unit} ,1) # time multiplyer\n",
        "    AS T\n",
        "FROM\n",
        "  transactions\n",
        "GROUP BY\n",
        "  1\n",
        "  \n",
        "\"\"\".format(**dc)\n",
        "\n",
        "\n",
        "q2 = \"\"\"\n",
        "\n",
        "WITH transactions as(\n",
        "SELECT\n",
        "  cds.value as user_id,\n",
        "  PARSE_DATE('%Y%m%d', date) AS transaction_date,\n",
        "  SUM(totals.totalTransactionRevenue) / 1000000 AS transaction_value\n",
        "FROM\n",
        "  `{table_name}`,\n",
        "  UNNEST(customdimensions) AS cds\n",
        "WHERE\n",
        "  _TABLE_SUFFIX BETWEEN '{start_date}'\n",
        "  AND '{end_date}'\n",
        "  AND cds.index = {user_id_dimension_index}\n",
        "  AND totals.transactions > 0\n",
        "GROUP BY\n",
        "  1,2)\n",
        "  \n",
        "SELECT\n",
        "  user_id,\n",
        "  COUNT(1) AS total_transactions,\n",
        "  ROUND(SUM(transaction_value)/COUNT(1),1) AS average_order_value,\n",
        "  (COUNT(1)-1) AS frequency,  \n",
        "  ROUND (DATE_DIFF(MAX(transaction_date),\n",
        "            MIN(transaction_date),\n",
        "            DAY) / 7 ,1) # time multiplyer \n",
        "    AS recency,\n",
        "\n",
        "  ROUND((DATE_DIFF((SELECT MAX(transaction_date) FROM transactions),\n",
        "             MIN(transaction_date),\n",
        "             DAY)+1) / 7 ,1) # time multiplyer\n",
        "    AS T\n",
        "FROM\n",
        "  transactions\n",
        "GROUP BY\n",
        "  1\n",
        "\"\"\".format(**dc)\n",
        "\n",
        "if id_type == 'user_id' and user_id_dimension_index != 0:\n",
        "  q = q2\n",
        "else:\n",
        "  q = q1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJgr69Q660q8"
      },
      "source": [
        "df = pd.io.gbq.read_gbq(q, project_id=project_id, verbose=False, dialect='standard') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jijd7nI9ppt9"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkjAd28SccGM"
      },
      "source": [
        "### Train the model\n",
        "Using the transformed dataset we fit the BetaGeoFitter model. Once the model is trained we can plot probability of alive Matrix.\n",
        "\n",
        "Once the BetaGeofitter model is trained, we can train a Gamma Model to estimate future average order value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4qvnRbaXZTv"
      },
      "source": [
        "bgf = BetaGeoFitter(penalizer_coef=0.0)\n",
        "bgf.fit(df['frequency'], df['recency'], df['T'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2UDtwNlzPnp"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plot_frequency_recency_matrix(bgf)\n",
        ";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9UxML7cot6G"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plot_probability_alive_matrix(bgf)\n",
        ";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW4AeDa909HC"
      },
      "source": [
        "ggf = GammaGammaFitter(penalizer_coef = 0)\n",
        "ggf.fit(df['total_transactions'],\n",
        "        df['average_order_value'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oeF7L384uPW"
      },
      "source": [
        "df['prob_alive'] = bgf.conditional_probability_alive(df['frequency'], df['recency'], df['T'])\n",
        "df['predicted_transactions'] = bgf.conditional_expected_number_of_purchases_up_to_time(units_to_predict, df['frequency'], df['recency'], df['T'])\n",
        "df['predicted_aov'] = ggf.conditional_expected_average_profit(df['total_transactions'], df['average_order_value'])\n",
        "df['predicted_ltv'] = df['predicted_transactions'] * df['predicted_aov']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DFYUqmSdxIN"
      },
      "source": [
        "### Results\n",
        "\n",
        "In this section we display the following by user:\n",
        "\n",
        "\n",
        "\n",
        "*   **prob_alive**: The probability of a customer being alive\n",
        "*   **predicted_transactions**: The predicted number of transactions in the predicted period\n",
        "*   **predicted_aov**: The predicted average order value in the predicted period\n",
        "*   **predicted_ltv**: The total customer life time value for the predicted period. (predicted_ltv = predicted_aov * predicted_transactions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We also group the customer into N segments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqjNkQKeb8ma"
      },
      "source": [
        "df['segment'] = pd.qcut(df['predicted_ltv'], number_of_segments, labels=list(range(1,number_of_segments+1)))\n",
        "summary = df.groupby('segment').agg({'prob_alive':'mean', 'predicted_transactions': 'mean', \n",
        "                           'predicted_aov': 'mean', \n",
        "                           'predicted_ltv': ['mean','sum']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj5mrsZFY9-t"
      },
      "source": [
        "summary = summary.round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p0YF73yY-sX"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJA_d-YATHUp"
      },
      "source": [
        "data_import_key_index = 'ga:dimension{}'.format(data_import_key_index)\n",
        "data_import_value_index = 'ga:dimension{}'.format(data_import_value_index)\n",
        "\n",
        "df = df[['user_id', 'segment']] \n",
        "df.columns = [data_import_key_index, data_import_value_index]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJXFNdBTUNg2"
      },
      "source": [
        "# Optional: Save output to drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vekNf6jon4S5"
      },
      "source": [
        "def output_to_googledrive(df, output_file_name='ltv.csv'):\n",
        "    date = str(datetime.datetime.today()).split()[0]\n",
        "    file_name = date + '_' + output_file_name\n",
        "    file_url = 'https://drive.google.com/open?id='\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    uploaded = drive.CreateFile({'title': file_name})\n",
        "    uploaded.SetContentString(df.to_csv(index=False))\n",
        "    uploaded.Upload()\n",
        "    print(file_name)\n",
        "    print('Full File URL: {}{}'.format(file_url, uploaded.get('id')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DXLzt8mnQ3v"
      },
      "source": [
        "output_to_googledrive(df, output_file_name='full_ltv.csv')\n",
        "output_to_googledrive(summary, output_file_name='summary_ltv.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z57pTY1KSN0t"
      },
      "source": [
        "### Sending data back to Google Analytics 360\n",
        "\n",
        "We have 2 approaches to upload information back to GA.\n",
        "\n",
        "\n",
        "1.   **Measurement Protocol Hits**: http call to our collection servers passing the ltv and segmentation per user.\n",
        "2.   **Data Import (Query Time)**: Upload a csv into Google Analytics 360 using the management API our the UI.\n",
        "\n"
      ]
    }
  ]
}